# -*- coding: utf-8 -*-
"""HAND GESTURE RECOGNITION PROJECT IPMV

Automatically generated by Colab.

Original file is located at
Â  Â  https://colab.research.google.com/drive/113NdmFiv3__iB_46HjUx9XpKeAEyRVdh
"""

#INSTALLING MEDIAPIPE EXTENSION
!python -m pip install mediapipe

#INSTALLING MEDIAPIPE NUMPY EXTENSION
!pip install opencv-python mediapipe numpy

#INSTALLING MEDIAPIPE AND OPENCV EXTENSION
!pip install opencv-python opencv-contrib-python mediapipe numpy

#INSTALLING PANDAS MATPLOTLIB EXTENSION
!pip install matplotlib pandas scikit-learn

#INSTALLING LATEST VERSION OF MEDIAPIPE
!pip install mediapipe==0.10.11

!pip install mediapipe

# STEP 1: Install dependencies
!pip install mediapipe==0.10.11 opencv-python pyngrok==4.1.1

# STEP 2: Import required libraries
import cv2
import mediapipe as mp
import numpy as np
from google.colab.output import eval_js
from base64 import b64decode
from google.colab.patches import cv2_imshow
from IPython.display import display, Javascript

# STEP 3: JavaScript Webcam Capture for Google Colab
def capture_image():
Â  Â  js = Javascript('''
Â  Â  async function captureImage() {
Â  Â  Â  Â  const div = document.createElement('div');
Â  Â  Â  Â  document.body.appendChild(div);
Â  Â  Â  Â  const video = document.createElement('video');
Â  Â  Â  Â  div.appendChild(video);
Â  Â  Â  Â  video.style.display = 'block';

Â  Â  Â  Â  const stream = await navigator.mediaDevices.getUserMedia({video: true});
Â  Â  Â  Â  video.srcObject = stream;
Â  Â  Â  Â  await video.play();

Â  Â  Â  Â  google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);

Â  Â  Â  Â  return new Promise((resolve) => {
Â  Â  Â  Â  Â  Â  const canvas = document.createElement('canvas');
Â  Â  Â  Â  Â  Â  canvas.width = video.videoWidth;
Â  Â  Â  Â  Â  Â  canvas.height = video.videoHeight;
Â  Â  Â  Â  Â  Â  canvas.getContext('2d').drawImage(video, 0, 0);
Â  Â  Â  Â  Â  Â  stream.getVideoTracks()[0].stop();
Â  Â  Â  Â  Â  Â  resolve(canvas.toDataURL('image/jpeg'));
Â  Â  Â  Â  });
Â  Â  }
Â  Â  ''')
Â  Â  display(js)
Â  Â  data = eval_js('captureImage()')
Â  Â  binary = b64decode(data.split(',')[1])
Â  Â  np_arr = np.frombuffer(binary, dtype=np.uint8)
Â  Â  img = cv2.imdecode(np_arr, cv2.IMREAD_COLOR)
Â  Â  return img

# STEP 4: Setup MediaPipe Hands
mp_hands = mp.solutions.hands
mp_drawing = mp.solutions.drawing_utils

# STEP 5: Define gesture recognition logic with improved accuracy
def recognize_gesture(landmarks):
Â  Â  tips_ids = [4, 8, 12, 16, 20]
Â  Â  finger_states = []

Â  Â  for tip_id in tips_ids[1:]:
Â  Â  Â  Â  tip = landmarks.landmark[tip_id].y
Â  Â  Â  Â  pip = landmarks.landmark[tip_id - 2].y
Â  Â  Â  Â  finger_states.append(tip < pip)

Â  Â  thumb_tip = landmarks.landmark[4]
Â  Â  thumb_ip = landmarks.landmark[3]
Â  Â  thumb_state = thumb_tip.x > thumb_ip.x

Â  Â  if all(finger_states) and thumb_state:
Â  Â  Â  Â  return "Open Palm ğŸ–"
Â  Â  elif finger_states[0] and finger_states[1] and not any(finger_states[2:]):
Â  Â  Â  Â  return "Peace âœŒ"
Â  Â  elif not any(finger_states) and not thumb_state:
Â  Â  Â  Â  return "Fist âœŠ"
Â  Â  elif not any(finger_states) and thumb_state:
Â  Â  Â  Â  return "Thumbs Up ğŸ‘"
Â  Â  elif finger_states[0] and not any(finger_states[1:]) and not thumb_state:
Â  Â  Â  Â  return "Pointing â˜ï¸"
Â  Â  else:
Â  Â  Â  Â  return "Unknown Gesture ğŸ¤”"

# STEP 6: Capture an Image and Detect Gesture
with mp_hands.Hands(
Â  Â  static_image_mode=True,
Â  Â  max_num_hands=1,
Â  Â  min_detection_confidence=0.7) as hands:
Â  Â  try:
Â  Â  Â  Â  frame = capture_image()
Â  Â  Â  Â  frame = cv2.flip(frame, 1)
Â  Â  Â  Â  image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
Â  Â  Â  Â  results = hands.process(image)
Â  Â  Â  Â  image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)

Â  Â  Â  Â  if results.multi_hand_landmarks:
Â  Â  Â  Â  Â  Â  for hand_landmarks in results.multi_hand_landmarks:
Â  Â  Â  Â  Â  Â  Â  Â  mp_drawing.draw_landmarks(image, hand_landmarks, mp_hands.HAND_CONNECTIONS)
Â  Â  Â  Â  Â  Â  Â  Â  gesture = recognize_gesture(hand_landmarks)
Â  Â  Â  Â  Â  Â  Â  Â  cv2.putText(image, f"Gesture: {gesture}", (50, 50),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  gesture = "No hand detected"
Â  Â  Â  Â  Â  Â  cv2.putText(image, gesture, (50, 50),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)

Â  Â  Â  Â  cv2_imshow(image)
Â  Â  except Exception as e:
Â  Â  Â  Â  print(f"Error: {e}")


